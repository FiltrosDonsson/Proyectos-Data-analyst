{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1662cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Corregir la ruta del archivo\n",
    "df = pd.read_parquet(\"/home/donsson/proyectos/API/ventashistoricas56semanas.parquet\") #movimiento  facturas\n",
    "df_p = pd.read_parquet(\"/home/donsson/proyectos/API/costo_productos.parquet\") #Costos unitarios\n",
    "df_vp = pd.read_parquet(\"/home/donsson/proyectos/API/ventas_perdidas_2025.parquet\") #ventas perdidas\n",
    "vp_reales = pd.read_excel(\"/home/donsson/proyectos/INDICADOR NS/vp_agosto.xlsx\") #vp reales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62b47c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3670d2e",
   "metadata": {},
   "source": [
    "## Facturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Diccionario de códigos a sucursales\n",
    "mapa_codigos = {\n",
    "    \"FCAL\": \"SUCURSAL CALI\",\n",
    "    \"FMED\": \"SUCURSAL MEDELLIN\",\n",
    "    \"FMDE\":\"SUCURSAL MEDELLIN\",\n",
    "    \"FCTG\": \"SUCURSAL CARTAGENA\",\n",
    "    \"FBAQ\": \"SUCURSAL BARRANQUILLA\",\n",
    "    \"FVAL\": \"SUCURSAL VALLADOLID\",\n",
    "    \"FCOT\":\"PRINCIPAL COTA\",\n",
    "    \"FBUC\":\"SUCURSAL BUCARAMANGA\",\n",
    "    \"FNOR\":\"SUCURSAL NORTE\",\n",
    "    \"FCL6\":\"SUCURSAL CALLE 6\",\n",
    "    \"PV2E\":\"SUCURSAL CALLE 6\",\n",
    "    \"PV3E\":\"SUCURSAL VALLADOLID\",\n",
    "    \"CLL6\":\"SUCURSAL CALLE 6\",\n",
    "    \"PV1E\":\"SUCURSAL COTA\" ,#Las que comienzan por p son los mostradores\n",
    "    \"PV4E\":\"SUCURSAL NORTE\",\n",
    "    \"PV9E\":\"SUCURSAL CALI\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Equivalencias para normalizar nombres truncados o mal escritos\n",
    "mapa_equivalencias = {\n",
    "    \"MEDELLIN\": \"SUCURSAL MEDELLIN\",\n",
    "    \"MEDELLI\": \"SUCURSAL MEDELLIN\",\n",
    "    \"MEDELL\": \"SUCURSAL MEDELLIN\",\n",
    "    \"MEDELI\": \"SUCURSAL MEDELLIN\",\n",
    "    \"CALI\": \"SUCURSAL CALI\",\n",
    "    \"CLL6\":\"SUCURSAL CALLE 6\",\n",
    "    \"BUCARAMANGA\":\"SUCURSAL BUCARAMANGA\",\n",
    "    \"BARRANQUILLA\": \"SUCURSAL BARRANQUILLA\",\n",
    "    \"VALLADOLID\": \"SUCURSAL VALLADOLID\",\n",
    "    \"CALLE 6\":\"SUCURSAL CALLE 6\",\n",
    "    \"COTA\":\"PRINCIPAL COTA\",\n",
    "    \"NORTE\":\"SUCURSAL NORTE\"\n",
    "}\n",
    "\n",
    "def normalizar(texto):\n",
    "    \"\"\"Quita tildes y pasa a mayúsculas\"\"\"\n",
    "    texto = unicodedata.normalize(\"NFKD\", texto)\n",
    "    texto = \"\".join([c for c in texto if not unicodedata.combining(c)])\n",
    "    return texto.upper()\n",
    "\n",
    "def extraer_sucursal(nombre):\n",
    "    if not isinstance(nombre, str):\n",
    "        return \"VENDEDOR EXTERNO\"\n",
    "    \n",
    "    sucursal = None\n",
    "    \n",
    "    # 1) Buscar \"Mostrador ...\"\n",
    "    match = re.search(r\"Mostrador\\s+([A-Za-z0-9\\s]+)\", nombre, re.IGNORECASE)\n",
    "    if match:\n",
    "        sucursal = match.group(1).strip()\n",
    "    else:\n",
    "        # 2) Buscar \"Calle\" o \"Cota\"\n",
    "        match2 = re.search(r\"(Calle\\s+\\d+|Cota)\", nombre, re.IGNORECASE)\n",
    "        if match2:\n",
    "            sucursal = match2.group(1).strip()\n",
    "        else:\n",
    "            # 3) Buscar prefijo de código\n",
    "            for prefijo, ciudad in mapa_codigos.items():\n",
    "                if nombre.upper().startswith(prefijo):\n",
    "                    return ciudad\n",
    "            return \"VENDEDOR EXTERNO\"\n",
    "    \n",
    "    # Normalizar texto\n",
    "    sucursal = normalizar(sucursal)\n",
    "    \n",
    "    # Limpiar T1, T2, T3 al final\n",
    "    sucursal = re.sub(r\"\\s*T\\d+$\", \"\", sucursal).strip()\n",
    "    \n",
    "    # Aplicar equivalencias\n",
    "    sucursal = mapa_equivalencias.get(sucursal, sucursal)\n",
    "    \n",
    "    return sucursal\n",
    "\n",
    "# Aplicar al dataframe\n",
    "df[\"Sucursal\"] = df[\"invoice_name\"].apply(extraer_sucursal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25376774",
   "metadata": {},
   "source": [
    "## Ventas perdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# Filtrar almacenamiento agotado\n",
    "# ===============================\n",
    "df_vp = df_vp[df_vp[\"almacenamiento_tipo\"].str.lower() == \"agotado\"]\n",
    "\n",
    "# ===============================\n",
    "# Asegurar tipos correctos\n",
    "# ===============================\n",
    "df_vp = df_vp.copy()\n",
    "df_vp[\"fecha\"] = pd.to_datetime(df_vp[\"fecha\"], errors=\"coerce\")\n",
    "\n",
    "# Numéricos\n",
    "for col in [\"cantidad\", \"cantidad_existencia\", \"cantidad_reservada\"]:\n",
    "    df_vp[col] = pd.to_numeric(df_vp[col], errors=\"coerce\").fillna(0).clip(lower=0)\n",
    "\n",
    "# ===============================\n",
    "# Reglas Odoo vectorizadas\n",
    "# ===============================\n",
    "is_cot = df_vp[\"origen\"].fillna(\"\").str.lower() == \"cotizacion\"\n",
    "ignore_mask = df_vp[\"cantidad\"] >= 100\n",
    "\n",
    "ajuste = np.where(\n",
    "    is_cot,\n",
    "    df_vp[\"cantidad\"] - df_vp[\"cantidad_existencia\"] - df_vp[\"cantidad_reservada\"],\n",
    "    df_vp[\"cantidad\"] - df_vp[\"cantidad_reservada\"]\n",
    ")\n",
    "\n",
    "# Aplicar reglas de descarte y piso en cero\n",
    "ajuste = np.where(ignore_mask, 0, ajuste)\n",
    "ajuste = np.where(ajuste > 0, ajuste, 0)\n",
    "\n",
    "df_vp[\"ventas_perdidas\"] = ajuste.astype(float)\n",
    "\n",
    "# ===============================\n",
    "# Columnas temporales\n",
    "# ===============================\n",
    "df_vp[\"Semana\"] = df_vp[\"fecha\"].dt.to_period(\"W\").dt.start_time\n",
    "df_vp[\"ano\"]   = df_vp[\"Semana\"].dt.year\n",
    "df_vp[\"mes\"]   = df_vp[\"Semana\"].dt.month\n",
    "df_vp[\"dia\"]   = df_vp[\"Semana\"].dt.day\n",
    "\n",
    "# ===============================\n",
    "# Filtro adicional: excluir SERV y CARCASA\n",
    "# ===============================\n",
    "mask_excluir = ~df_vp[\"product_ref\"].str.contains(\"SERV|CARCASA\", case=False, na=False)\n",
    "df_vp = df_vp[mask_excluir]\n",
    "\n",
    "# ===============================\n",
    "# Agrupación por tienda + producto + semana\n",
    "# ===============================\n",
    "lost_by_week = (\n",
    "    df_vp.groupby([\"store_name\", \"product_ref\", \"Semana\", \"ano\", \"mes\", \"dia\"], as_index=False)[\"ventas_perdidas\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"ventas_perdidas\": \"lost_sales\"})\n",
    ")\n",
    "\n",
    "# Mostrar resultado agrupado\n",
    "vp_week = lost_by_week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_reales[\"product_ref\"] = vp_reales[\"Descripcion\"].str.extract(r\"\\[([A-Z0-9]+)\\]\")\n",
    "vp_reales.head()\n",
    "\n",
    "# Asegurar que ambos son strings para evitar problemas\n",
    "vp_week[\"product_ref\"] = vp_week[\"product_ref\"].astype(str)\n",
    "vp_reales[\"product_ref\"] = vp_reales[\"product_ref\"].astype(str)\n",
    "\n",
    "# 1. Obtener listas únicas\n",
    "refs_week = set(vp_week[\"product_ref\"].unique())\n",
    "refs_real = set(vp_reales[\"product_ref\"].unique())\n",
    "\n",
    "# 2. Diferencia: los que están en vp_week pero no en vp_real\n",
    "refs_extra = refs_week - refs_real\n",
    "\n",
    "# 3. Filtrar el dataframe para verlos completos\n",
    "df_discrepantes = vp_week[vp_week[\"product_ref\"].isin(refs_extra)]\n",
    "\n",
    "\n",
    "df_discrepantes = df_discrepantes[(df_discrepantes[\"mes\"]==8) & (df_discrepantes[\"lost_sales\"]>0) ]\n",
    "df_discrepantes = df_discrepantes.groupby(\"product_ref\").agg({\"lost_sales\":\"sum\"})\n",
    "print(\"Cantidad de vp que no deberia tomar:\", df_discrepantes[\"lost_sales\"].sum())\n",
    "df_discrepantes #Los productos que no se movieron hace mucho tiempo no salen en el analisis de ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_agosot_2025 = vp_week[(vp_week[\"ano\"]==2025) & (vp_week[\"mes\"]==8) ]\n",
    "vp_agosot_2025.to_excel(\"vp_revisar.xlsx\")\n",
    "\n",
    "vp_agosot_2025.query(\"product_ref == 'DAB28118025' and store_name == 'PRINCIPAL COTA'\")\n",
    "#vp_agosot_2025.groupby(\"store_name\")[\"lost_sales\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd2d4c",
   "metadata": {},
   "source": [
    "# UNION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07df280",
   "metadata": {},
   "source": [
    "## EMA SEMANAL CON VP SEMANALES (SOLO 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4dcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db76ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Procesar ventas normales\n",
    "# ===============================\n",
    "df_sales = df.copy()\n",
    "df_sales[\"date_invoice\"] = pd.to_datetime(df_sales[\"date_invoice\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Referncia de producto\n",
    "df_sales[\"product_ref\"] = df_sales[\"product_name\"].str.extract(r\"\\[([A-Z0-9]+)\\]\")\n",
    "\n",
    "\n",
    "# Columnas temporales igual que en df_vp\n",
    "df_sales[\"Semana\"] = df_sales[\"date_invoice\"].dt.to_period(\"W\").dt.start_time\n",
    "df_sales[\"ano\"]    = df_sales[\"Semana\"].dt.year\n",
    "df_sales[\"mes\"]    = df_sales[\"Semana\"].dt.month\n",
    "df_sales[\"dia\"]    = df_sales[\"Semana\"].dt.day\n",
    "\n",
    "# ===============================\n",
    "# Agrupación por tienda + producto + semana\n",
    "# ===============================\n",
    "sales_by_week = (\n",
    "    df_sales.groupby([\"Sucursal\", \"product_ref\", \"Semana\", \"ano\", \"mes\", \"dia\"], as_index=False)[\"quantity\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"quantity\": \"sales\",\n",
    "                     \"Sucursal\":\"store_name\"})\n",
    ")\n",
    "\n",
    "# Resultado\n",
    "sales_by_week.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4fd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(\n",
    "    sales_by_week[[\"store_name\", \"product_ref\", \"Semana\", \"sales\"]],\n",
    "    vp_week[[\"store_name\", \"product_ref\", \"Semana\", \"lost_sales\"]],\n",
    "    on=[\"store_name\", \"product_ref\", \"Semana\"],\n",
    "    how=\"outer\"\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bffdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"año\"]    = df_merged[\"Semana\"].dt.year\n",
    "df_merged[\"mes\"]    = df_merged[\"Semana\"].dt.month\n",
    "df_merged[\"dia\"]    = df_merged[\"Semana\"].dt.day\n",
    "\n",
    "df_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a4852",
   "metadata": {},
   "source": [
    "## NORMALIZAR DF DE COSTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95373c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p[\"product_ref\"] = df_p[\"product_name\"].str.extract(r\"\\[([A-Z0-9]+)\\]\")\n",
    "\n",
    "df_p_unique = (\n",
    "    df_p[[\"product_ref\", \"producto_costo_unitario\"]]\n",
    "    .drop_duplicates(subset=[\"product_ref\"])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddf7d7",
   "metadata": {},
   "source": [
    "### UNIR COSTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_def = pd.merge(\n",
    "    df_merged,\n",
    "    df_p_unique,\n",
    "    on=\"product_ref\",\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "merge_def = df_merge_def[df_merge_def[\"producto_costo_unitario\"] !=0].copy() #Eliminar productos sin costos unitarios\n",
    "\n",
    "merge_def.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compute_demand_and_ema(df,\n",
    "                           alpha=0.20,        # 20% como en tu config\n",
    "                           n_init_weeks=12,   # semanas que usa el proceso (rango de evaluacion)\n",
    "                           week_col=\"Semana\",\n",
    "                           sales_col=\"sales\",\n",
    "                           lost_col=\"lost_sales\"):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ---------- Asegurar tipos y semana iniciando lunes ----------\n",
    "    # Si Semana no es datetime, intentamos convertir\n",
    "    df[week_col] = pd.to_datetime(df[week_col], errors=\"coerce\")\n",
    "\n",
    "    # Normalizar semanas al lunes inicio (start of week, lunes)\n",
    "    # Esto genera el timestamp del lunes de la semana ISO correspondiente\n",
    "    # (equivalente al comportamiento del código original)\n",
    "    df[week_col] = df[week_col].dt.to_period('W-MON').dt.start_time\n",
    "\n",
    "    # Asegurar numéricos\n",
    "    df[sales_col] = pd.to_numeric(df[sales_col], errors=\"coerce\").fillna(0)\n",
    "    df[lost_col]  = pd.to_numeric(df[lost_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Orden y agrupación\n",
    "    df = df.sort_values([\"store_name\", \"product_ref\", week_col])\n",
    "\n",
    "    out_groups = []\n",
    "\n",
    "    # Recorremos por tienda+producto\n",
    "    for (store, prod), g in df.groupby([\"store_name\", \"product_ref\"], sort=False):\n",
    "        g = g.sort_values(week_col).reset_index(drop=True)\n",
    "        sales = g[sales_col].to_numpy(dtype=float)\n",
    "        lost  = g[lost_col].to_numpy(dtype=float)\n",
    "\n",
    "        L = len(g)\n",
    "        demanda = np.zeros(L, dtype=float)\n",
    "        ema_arr = np.zeros(L, dtype=float)\n",
    "\n",
    "        # Inicial EMA (EMA_0) -> si hay suficientes semanas, usamos el promedio de las primeras n_init_weeks sales\n",
    "        # Si hay menos semanas, usamos media de las sales disponibles.\n",
    "        if L == 0:\n",
    "            out_groups.append(g)\n",
    "            continue\n",
    "\n",
    "        # inicialización: usar promedio de 'sales' de las primeras n_init_weeks (o de lo que haya)\n",
    "        init_n = min(n_init_weeks, L)\n",
    "        # si no hay sales (todos ceros), ema_prev será 0\n",
    "        ema_prev = float(np.nanmean(sales[:init_n])) if init_n > 0 else 0.0\n",
    "        if np.isnan(ema_prev):\n",
    "            ema_prev = 0.0\n",
    "\n",
    "        # Iterar semanas y aplicar reglas del documento\n",
    "        for i in range(L):\n",
    "            s = sales[i]\n",
    "            l = lost[i]\n",
    "\n",
    "            # Regla 1: si ventas >= 2 * ventas_perdidas\n",
    "            if s >= 2.0 * l:\n",
    "                demand_candidate = s + l\n",
    "                # aplicar tope MAX = 1.5 * ventas\n",
    "                # (si s == 0 ese caso no entra porque s >= 2*l sería falso cuando l>0)\n",
    "                demand = min(demand_candidate, 1.5 * s) if s > 0 else demand_candidate\n",
    "            else:\n",
    "                # Regla 2: ventas < 2 * ventas_perdidas\n",
    "                # demanda = ventas + 0.5 * EMA(t-1)\n",
    "                demand = s + 0.5 * ema_prev\n",
    "\n",
    "            # Guardar demanda\n",
    "            demanda[i] = demand\n",
    "\n",
    "            # Calcular EMA (iterativo) con alpha\n",
    "            ema = alpha * demand + (1.0 - alpha) * ema_prev\n",
    "            ema =round(ema,2)\n",
    "            ema_arr[i] = ema\n",
    "\n",
    "            # actualizar para la próxima semana\n",
    "            ema_prev = ema\n",
    "\n",
    "        # Añadir columnas al grupo\n",
    "        g = g.copy()\n",
    "        g[\"demanda_ajustada\"] = demanda\n",
    "        g[\"EMA\"] = ema_arr\n",
    "\n",
    "        out_groups.append(g)\n",
    "\n",
    "    # Concat y devolver\n",
    "    result = pd.concat(out_groups, ignore_index=True, sort=False)\n",
    "\n",
    "    # Mantener mismo orden original\n",
    "    result = result.sort_values([\"store_name\", \"product_ref\", week_col]).reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "# ------------------ USO ------------------\n",
    "# suponiendo merge_def es tu df final\n",
    "# ajusta alpha y n_init_weeks si quieres (alpha=0.2, n_init_weeks=12 por defecto)\n",
    "df_with_demand = compute_demand_and_ema(merge_def, alpha=0.20, n_init_weeks=12)\n",
    "\n",
    "# ver primeras filas\n",
    "df_with_demand[[\"store_name\",\"product_ref\",\"Semana\",\"sales\",\"lost_sales\",\"demanda_ajustada\",\"EMA\"]].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bc9618",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_demand[\"semana_num\"] = df_with_demand[\"Semana\"].dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37534d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_demand_2025 = df_with_demand[df_with_demand[\"año\"]==2025]\n",
    "\n",
    "\n",
    "demand_2025 = df_demand_2025[[\"store_name\",\"product_ref\",\"año\",\"semana_num\",\"EMA\",\"producto_costo_unitario\",\"demanda_ajustada\"]]\n",
    "\n",
    "filtro_bq = demand_2025[(demand_2025[\"store_name\"]==\"SUCURSAL BARRANQUILLA\") & (demand_2025[\"semana_num\"]==36)].sort_values(by=(\"EMA\"),ascending=False)\n",
    "\n",
    "filtro_bq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ---------- Funciones auxiliares ----------\n",
    "\n",
    "def get_first_day_of_week(year: int) -> datetime:\n",
    "    \"\"\"\n",
    "    Reproduce la lógica del Odoo original:\n",
    "    - Si el 1 de enero cae jueves o después → semana 1 empieza en el siguiente lunes\n",
    "    - Si cae lunes, martes o miércoles → retrocede al lunes anterior\n",
    "    \"\"\"\n",
    "    jan1 = datetime(year, 1, 1)\n",
    "    dow = jan1.weekday()  # lunes=0, domingo=6\n",
    "    if dow >= 3:  # jueves (3), viernes (4), sábado (5), domingo (6)\n",
    "        # semana 1 arranca el lunes siguiente\n",
    "        return jan1 + timedelta(days=(7 - dow))\n",
    "    else:\n",
    "        # semana 1 arranca el lunes anterior o el mismo día si ya es lunes\n",
    "        return jan1 - timedelta(days=dow)\n",
    "\n",
    "\n",
    "def get_start_end_week(year: int, week_num: int):\n",
    "    \"\"\"Devuelve lunes y domingo de la semana solicitada, siguiendo get_first_day_of_week\"\"\"\n",
    "    first_monday = get_first_day_of_week(year)\n",
    "    start_date = first_monday + timedelta(weeks=week_num - 1)\n",
    "    end_date = start_date + timedelta(days=6)\n",
    "    return start_date, end_date\n",
    "\n",
    "\n",
    "# ---------- Función principal ----------\n",
    "\n",
    "def compute_demand_and_ema(df,\n",
    "                           alpha=0.20,\n",
    "                           n_init_weeks=12,\n",
    "                           week_col=\"Semana\",\n",
    "                           sales_col=\"sales\",\n",
    "                           lost_col=\"lost_sales\"):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalizar semanas con la misma lógica Odoo (get_start_end_week)\n",
    "    df[week_col] = pd.to_datetime(df[week_col], errors=\"coerce\")\n",
    "    df[\"year\"] = df[week_col].dt.year\n",
    "    df[\"week\"] = df[week_col].dt.isocalendar().week\n",
    "\n",
    "    # Reemplazar la semana por el lunes base según Odoo\n",
    "    df[week_col] = df.apply(\n",
    "        lambda r: get_start_end_week(int(r[\"year\"]), int(r[\"week\"]))[0],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Asegurar numéricos\n",
    "    df[sales_col] = pd.to_numeric(df[sales_col], errors=\"coerce\").fillna(0)\n",
    "    df[lost_col]  = pd.to_numeric(df[lost_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # Orden\n",
    "    df = df.sort_values([\"store_name\", \"product_ref\", week_col])\n",
    "\n",
    "    out_groups = []\n",
    "\n",
    "    for (store, prod), g in df.groupby([\"store_name\", \"product_ref\"], sort=False):\n",
    "        g = g.sort_values(week_col).reset_index(drop=True)\n",
    "        sales = g[sales_col].to_numpy(dtype=float)\n",
    "        lost  = g[lost_col].to_numpy(dtype=float)\n",
    "\n",
    "        L = len(g)\n",
    "        demanda = np.zeros(L, dtype=float)\n",
    "        ema_arr = np.zeros(L, dtype=float)\n",
    "\n",
    "        if L == 0:\n",
    "            out_groups.append(g)\n",
    "            continue\n",
    "\n",
    "        # --- Paso 1: calcular demandas de las primeras n_init_weeks\n",
    "        init_n = min(n_init_weeks, L)\n",
    "        demanda_inicial = []\n",
    "\n",
    "        ema_prev = 0.0\n",
    "        for i in range(init_n):\n",
    "            s, l = sales[i], lost[i]\n",
    "\n",
    "            if s >= 2.0 * l:\n",
    "                demand_candidate = s + l\n",
    "                demand = min(demand_candidate, 1.5 * s) if s > 0 else demand_candidate\n",
    "            else:\n",
    "                demand = s + 0.5 * ema_prev  # aquí ema_prev todavía es el inicial 0 para i=0\n",
    "\n",
    "            demanda[i] = demand\n",
    "            demanda_inicial.append(demand)\n",
    "            ema_prev = demand  # solo para el seed (no aplica alpha aquí)\n",
    "\n",
    "        # EMA inicial: promedio de demandas ajustadas\n",
    "        ema_prev = np.mean(demanda_inicial) if len(demanda_inicial) > 0 else 0.0\n",
    "        ema_prev = round(ema_prev, 2)\n",
    "\n",
    "        # Guardamos EMA inicial\n",
    "        ema_arr[:init_n] = ema_prev\n",
    "\n",
    "        # --- Paso 2: resto de semanas con recursión\n",
    "        for i in range(init_n, L):\n",
    "            s, l = sales[i], lost[i]\n",
    "\n",
    "            if s >= 2.0 * l:\n",
    "                demand_candidate = s + l\n",
    "                demand = min(demand_candidate, 1.5 * s) if s > 0 else demand_candidate\n",
    "            else:\n",
    "                demand = s + 0.5 * ema_prev\n",
    "\n",
    "            demanda[i] = demand\n",
    "            ema = alpha * demand + (1.0 - alpha) * ema_prev\n",
    "            ema = round(ema, 2)  # redondear cada semana\n",
    "            ema_arr[i] = ema\n",
    "            ema_prev = ema\n",
    "\n",
    "        # Añadir columnas\n",
    "        g = g.copy()\n",
    "        g[\"demanda_ajustada\"] = demanda\n",
    "        g[\"EMA\"] = ema_arr\n",
    "        out_groups.append(g)\n",
    "\n",
    "    result = pd.concat(out_groups, ignore_index=True, sort=False)\n",
    "    result = result.sort_values([\"store_name\", \"product_ref\", week_col]).reset_index(drop=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitivo = compute_demand_and_ema(merge_def,\n",
    "                           alpha=0.20,\n",
    "                           n_init_weeks=12,\n",
    "                           week_col=\"Semana\",\n",
    "                           sales_col=\"sales\",\n",
    "                           lost_col=\"lost_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitivo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_2025_2 = definitivo[definitivo[\"año\"]==2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_2025_2[\"semana_num\"] = demand_2025_2[\"Semana\"].dt.isocalendar().week\n",
    "\n",
    "demand_2025_2 = demand_2025_2[[\"store_name\",\"product_ref\",\"año\",\"semana_num\",\"EMA\",\"producto_costo_unitario\",\"demanda_ajustada\"]]\n",
    "\n",
    "filtro_bq_2 = demand_2025_2[(demand_2025_2[\"store_name\"]==\"SUCURSAL BARRANQUILLA\") & (demand_2025_2[\"semana_num\"]==36)].sort_values(by=(\"EMA\"),ascending=False)\n",
    "\n",
    "filtro_bq_2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bcfd40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
